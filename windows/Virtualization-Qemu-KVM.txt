QEMU
QEMU is a userland type 2 hypervisor for performing hardware virtualization (not to be confused with hardware-assisted virtualization), such as disk, network, VGA, PCI, USB, serial/parallel ports, etc. It is flexible in that it can emulate CPUs via dynamic binary translation (DBT) allowing code written for a given processor to be executed on another (i.e ARM on x86, or PPC on ARM). Though QEMU can run on its own and emulate all of the virtual machine’s resources, as all the emulation is performed in software it is extremely slow.

KVM
KVM is a Linux kernel module. It is a type 1 hypervisor that is a full virtualization solution for Linux on x86 hardware containing virtualization extensions (Intel VT or AMD-V). But what is full virtualization, you may ask? When a CPU is emulated (vCPU) by the hypervisor, the hypervisor has to translate the instructions meant for the vCPU to the physical CPU. As you can imagine this has a massive performance impact. To overcome this, modern processors support virtualization extensions, such as Intel VT-x and AMD-V. These technologies provide the ability for a slice of the physical CPU to be directly mapped to the vCPU. Therefore the instructions meant for the vCPU can be directly executed on the physical CPU slice.

Summary
QEMU can run independently, but due to the emulation being performed entirely in software it is extremely slow. To overcome this, QEMU allows you to use KVM as an accelerator so that the physical CPU virtualization extensions can be used. So to conclude: QEMU is a type 2 hypervisor that runs within user space and performs virtual hardware emulation, whereas KVM is a type 1 hypervisor that runs in kernel space, that allows a user space program access to the hardware virtualization features of various processors

Short overview of Image-Files formats
raw - is the default format. Allows flexible converting. Takes only used side on host, but only if used ext4 and (ext3?). This is called spare file or spare image
qed - is enhanced disc format for faster access (since QEMU 0.14). It supports overlay and sparse images. Overlay means that on create you can assign allready existing image as base and only differences will be written to the overlay image (Beware: base image has to stay unchanged!). Its also faster than qcow2
qcow2 - is most featured format in QEMU and is meant to replace qcow. This format support sparse images independent of underlying fs capabilities. It supports multiple VM-snapshots, encryption (AES) and compression.
qcowis - an old QEMU format. Images in qcow are sparse and like qcow2 independent of underlying file system capabilities.
vmdk - is standard format for VMware Workstation. Overlay function is similar to qcow2.
vdi - is standard format for Virtual Box.
parallels - is a standard image tp of virtualization solutions of Paralles Inc.
vpc - standard image format for Microsoft Virtual PC


[[Step 1]]: Check Virtualization Support in Ubuntu

Before installing KVM on Ubuntu, we are first going to verify if the hardware supports KVM. A minimum requirement for installing KVM is the availability of CPU virtualization extensions such as AMD-V and Intel-VT.

To check whether the Ubuntu system supports virtualization, run the following command. An outcome greater than 0 implies that virtualization is supported.
$ egrep -c '(vmx|svm)' /proc/cpuinfo

To check if your system supports KVM virtualization execute the command:
$ sudo kvm-ok

If the “kvm-ok” utility is not present on your server, install it by running the apt command:
$ sudo apt install cpu-checker

[[Step 2]]: Install KVM on Ubuntu 20.04 LTS

With the confirmation that our system can support KVM virtualization, we are going to install KVM, To install KVM, virt-manager, bridge-utils and other dependencies, run the command:
$ sudo apt install -y qemu qemu-kvm libvirt-daemon libvirt-clients bridge-utils virt-manager virt-viewer virtinst

A little explanation of the above packages.
- qemu package (quick emulator) is an application that allows you to perform hardware virtualization.
- qemu-kvm package is the main KVM package.
- libvritd-daemon is the virtualization daemon.
- libvirt-client is a package tht provides the virsh utility used for interacting with virtual machines.
- bridge-utils package helps you create a bridge connection to allow other users to access a virtual machine other than the host system.
- virt-manager is an application for managing virtual machines through a graphical user interface.
- virt-viewer is a utility that displays the graphical view for the virtual machine.
- virt-install is a utility that helps you to create virtual machines and install OS on those virtual machines from command line.
- virtinst is a set of command-line utilities for provisioning and modifying virtual machines.

Before proceeding further, we need to confirm that the virtualization daemon – libvritd-daemon – is running. To do so, execute the command.
$ sudo systemctl status libvirtd

You can enable it to start on boot by running:
$ sudo systemctl enable --now libvirtd

[[Step 3]]: Authorize Users to run Virtual Machines

Only members of the libvirt and kvm user groups can run virtual machines. Add a user to the libvirt & kvm group by typing:
$ sudo usermod -aG libvirt ${USER}
$ sudo usermod -aG kvm ${USER}

And grant the qemu the right to access the image folder by query the default setting 1st.
$ sudo getfacl -e /home/<folder>
$ sudo setfacl -m u:libvirt-qemu:rx /home/<folder>

Reload updated group membership info as follows. Upon asked for a password, enter your login password.
$ exec su -l $USER

[[Step 4]]: Verify the Installation

To check if the KVM modules are loaded, run the command:
$ lsmod | grep -i kvm

Confirm the installation was successful by using the virsh command:
$ sudo virsh list --all
$ sudo virsh net-list --all
$ sudo virsh net-info default
$ sudo virsh dominfo <vm>
$ sudo virsh domifaddr <vm>
$ sudo virsh domiflist <vm>

To start or power on the virtual machine, run:
$ sudo virsh start <vm>

To shut down a virtual machine use the syntax:
$ sudo virsh shutdown <vm>

To reboot the machine, run the command:
$ sudo virsh reboot <vm>

To destroy or forcefully power off virtual machine, run:
$ sudo virsh destroy <vm>

To delete the virtual machine along with its associated storage file, run:
$ sudo virsh undefine --domain <vm> --remove-all-storage

To edit a vm xml file, use:
$ export EDITOR=vim
$ sudo virsh edit <vm>

To connect to the guest console, use the command:
$ sudo virsh console <vm> --force

To create a new virtual machine with virsh, use:
$ virt-install --osinfo list
$ sudo virt-install \
  --name centos7 \
  --description "Test VM with CentOS 7" \
  --ram=1024 \
  --vcpus=2 \
  --os-type=Linux \
  --os-variant=rhel7 \
  --disk path=/var/lib/libvirt/images/centos7.qcow2,bus=virtio,size=10 \
  --graphics none \
  --location $HOME/iso/CentOS-7-x86_64-Everything-1611.iso \
  --network bridge:virbr0,model=virtio \
  --console pty,target_type=serial -x 'console=ttyS0,115200n8 serial'

[[[ Procedure to load and import existing disk image using commandline (without GUI) ]]]
Local cloud config setting guides - https://github.com/canonical/cloud-init/blob/main/doc/examples/cloud-config.txt

MiniStep 1: Create both the <user-data> and <meta-data> config files.
$ echo "#cloud-config
  # vim:syntax=yaml
  users:
  # A user by the name ec2-user is created in the image by default.
    - default
  # Following entry create user#1 and assigns password specified in plain text.
  # Please not use of plain text password is not recommended from security best
  # practises standpoint
    - name: <user#1>
      groups: sudo, users
      sudo: ALL=(ALL) NOPASSWD:ALL
      plain_text_passwd: <password>
      shell: /bin/bash
      lock_passwd: false
  # Following entry creates user#2 and attaches a hashed passwd to the user. 
  # Hashed passwords can be generated in multiple ways, example with python3 and the passlib module:
  # python3 -c 'import passlib.hash;print(passlib.hash.sha512_crypt.hash("password"))'
  # Newer versions of 'mkpasswd' will also work: mkpasswd -m sha-512 password
    - name: <user#2>
      groups: sudo, users
      sudo: ALL=(ALL) NOPASSWD:ALL
      passwd: <hashed-password>
      shell: /bin/bash
      lock_passwd: false
  chpasswd:
    users:
      - name: root
        password: <password>
        type: text
    expire: False
  " | tee /opt/virtualdrive/<image folder>/user-data

$ echo "#cloud-config
  local-hostname: <vm-hostname>
  # eth0 is the default network interface enabled in the image. You can
  # configure static network settings with an entry like below.
  network-interfaces: |
    iface enp1s0 inet static
    address 192.168.1.200
    network 192.168.1.0
    netmask 255.255.255.0
    broadcast 192.168.1.255
    gateway 192.168.1.254
  " | tee /opt/virtualdrive/<image folder>/meta-data

MiniStep 2: Generate an iso image w.r.t. the kernal images in qcow2 format for direct bootup.
To check the kvm image details and format.
$ qemu-img info /opt/virtualdrive/<image-folder>/linux-server-lts.img
To resize the image to a specific size in Gig.
$ qemu-img resize /opt/virtualdrive/<image-folder>/linux-server-lts.img <size>G
To generate the overideable iso format of the image.
$ genisoimage -output /opt/virtualdrive/<image-folder>/linux-server-lts.iso -volid cidata -joliet -rock user-data meta-data

MiniStep 3: Start up the image thru import into the KVM.
$ virt-install --import \
  --name <vm-name> \
  --os-type Linux \
  --os-variant ubuntu22.04 \
  --vcpus 2 --memory 2048 \
  --virt-type kvm \
  --disk path=/opt/virtualdrive/<image-folder>/linux-server-lts.img,device=disk,bus=virtio,size=10 \
  --disk path=/opt/virtualdrive/<image-folder>/linux-server-lts.iso,device=cdrom \
  --graphics none \
  --network bridge:virbr0,model=virtio

Once boot-up, create new user and configure network using netplan, below is sample basic netplan yaml file.
<netplan yaml file>
  network:
    version: 2
    ethernets:
      enp1s0:
        dhcp4: no
        dhcp6: no
        addresses:
        - 192.168.1.200/24
        nameservers:
          addresses:
          - 8.8.8.8
          - 8.8.4.4
          search:
          - myhome.local
        routes:
        - to: default
          via: 192.168.1.254

[[Step 5]]: The "default" Linux Bridge Network
When libvirt is in use and the libvirtd daemon is running, a 'default' network is created. Can verify that this network exists by using the virsh utility that comes with the libvirt-client package.

To invoke the utility so that it displays all the available virtual networks:
$ sudo virsh net-list --all

To obtain and edit the detailed information about the network:
$ sudo virsh net-edit <default>

# A temporary file containing the xml network definition is opened in a text editor
<network>
  <name>default</name>
  <uuid>168f6909-715c-4333-a34b-f74584d26328</uuid>
  <forward mode='nat'/>
  <bridge name='virbr0' stp='on' delay='0'/>
  <mac address='52:54:00:48:3f:0c'/>
  <ip address='192.168.122.1' netmask='255.255.255.0'>
    <dhcp>
      <range start='192.168.122.2' end='192.168.122.254'/>
    </dhcp>
  </ip>
</network>

The 'default' network is based on the use of the 'virbr0' virtual bridge, and uses NAT based connectivity to connect the virtual machines which are part of the network to the outside world.

To verify that the bridge exists using the ip command:
$ ip link show type bridge

To show the interfaces which are part of the bridge, and query only for interfaces which have the 'virbr0' bridge as master:
$ ip link show master <virbr0>

By default there is only one-interface attached to the bridge, this interface is a virtual-ethernet-interface: it is created and added to the bridge automatically, and its purpose is just to provide a stable MAC address (52:54:00:48:3f:0c in this case) for the bridge. Note: No physical interfaces should ever be added to the 'virbr0' bridge, since it uses NAT to provide connectivity.

[[Step 6]]: Use Bridged Networking for virtual machines
The default network provides a very straightforward way to achieve connectivity when creating virtual machines: everything is “ready” and works out of the box. Sometimes, however, we want to achieve a full bridgining connection, where the guest devices are connected to the host LAN, without using NAT, can achieve by creating a new bridge and share one of the host physical ethernet interfaces.

To create a new bridge & name this bridge 'br0':
$ sudo ip link add <br0> type bridge

To verify the bridge is created we do as before:
$ sudo ip link show type bridge

First we make sure the secondary interface state is UP:
$ sudo ip link set <eth1> up

- to add a host physical interface to the bridge. Notice that you can’t use your main ethernet interface in this case, since as soon as it is added to the bridge you would loose connectivity, since it will loose its IP address. In this case we will use an additional interface, 'eth1': this is an interface provided by an ethernet to a secondary network adapter attached to the machine.

To add a physical ethernet interface to the bridge:
$ sudo ip link set <eth1> master <br0>

To assign a static IP address to the bridge:
$ sudo ip address add dev <br0> <192.168.1.100/24>

To verify that the ip address was added to the interface:
$ ip addr show <br0>

[[[ Making the configuration persistent ]]]
To make our configuration persistent we must edit some configuration files, depending on the distribution we use.

{Debian and derivatives}
On the Debian family of distributions we must be sure that the bridge-utils package is installed:
$ sudo apt-get install bridge-utils

Once the package is installed, we should modify the content of the /etc/network/interfaces file:
  # This file describes the network interfaces available on your system
  # and how to activate them. For more information, see interfaces(5).

  # The loopback network interface
  auto lo
  iface lo inet loopback

  # Specify that the physical interface that should be connected to the bridge
  # should be configured manually, to avoid conflicts with NetworkManager
  iface <eth1> inet manual

  # The br0 bridge settings
  auto <br0>
  iface <br0> inet static
    bridge_ports <eth1>
      address <192.168.1.100>
      broadcast <192.168.1.255>
      netmask <255.255.255.0>
      gateway <192.168.1.1>

{Red Hat family of distributions}
To manipulate network scripts inside the /etc/sysconfig/network-scripts directory. Install the network-scripts package (NOT managed by NetworkManager):
$ sudo dnf install network-scripts

Once the package is installed, we need to create the file which will configure the <br0> bridge: /etc/sysconfig/network-scripts/ifcfg-br0. Inside the file we place the following content:
  DEVICE=<br0>
  TYPE=Bridge
  BOOTPROTO=none
  IPADDR=192.168.1.100
  GATEWAY=192.168.1.1
  NETMASK=255.255.255.0
  ONBOOT=yes
  DELAY=0
  NM_CONTROLLED=0

Than, we modify or create the file used to configure the physical interface we will connect to the bridge, in this case /etc/sysconfig/network-scripts/ifcfg-enp0s29u1u1:
  TYPE=ethernet
  BOOTPROTO=none
  NAME=<eth1>
  DEVICE=<eth1>
  ONBOOT=yes
  BRIDGE=<br0>
  DELAY=0
  NM_CONTROLLED=0

With our configurations ready, we can start the network service, and enable it at boot:
$ sudo systemctl enable --now network

[[[ Disabling netfilter for the bridge ]]]
To allow all traffic to be forwarded to the bridge, and therefore to the virtual machines connected to it, we need to disable netfilter. This is necessary, for example, for DNS resolution to work in the guest machines attached to the bridge. To do this we can create a file with the .conf extension inside the /etc/sysctl.d directory, let’s call it 99-netfilter-bridge.conf. Inside of it we write the following content:
  net.bridge.bridge-nf-call-ip6tables = 0
  net.bridge.bridge-nf-call-iptables = 0
  net.bridge.bridge-nf-call-arptables = 0

To load the settings written in the file, fist we ensure that the br_netfilter module is loaded:
$ sudo modprobe br_netfilter

To load the module automatically at boot, let’s create the /etc/modules-load.d/br_netfilter.conf file: it should contain only the name of the module itself:
  br_netfilter

Once the module is loaded, to load the settings we stored in the 99-netfilter-bridge.conf file, we can run:
$ sudo sysctl -p /etc/sysctl.d/99-netfilter-bridge.conf

[[[ Creating a new virtual network ]]]
At this point define a new “network” to be used by our virtual machines. Open a file with any editor and paste the following content, than save it as bridged-network.xml:
  <network>
    <name>bridged-network</name>
    <forward mode="bridge" />
    <bridge name="br0" />
  </network>

Once the file is ready we pass its position as argument to the net-define virsh subcommand:
$ sudo virsh net-define <bridged-network>.xml

To activate the new network and make so that it is auto-started, we should run:
$ sudo virsh net-start <bridged-network>
$ sudo virsh net-autostart <bridged-network>

We can verify the network has been activated by running the virsh net-list command, again:
$ sudo virsh net-list --all

We can now select the network by name when using the --network option:
$ virt-install --import \
  --name <vm-name> \
  --os-type Linux \
  --os-variant ubuntu22.04 \
  --vcpus 2 --memory 2048 \
  --virt-type kvm \
  --disk path=/opt/virtualdrive/<image-folder>/linux-server-lts.img,device=disk,bus=virtio,size=10 \
  --disk path=/opt/virtualdrive/<image-folder>/linux-server-lts.iso,device=cdrom \
  --graphics none \
  --network network=<bridged-network>

[[Step 7]]: Create Linux Bridge using brctl
If you don’t have networkmanager installed, you can use brctl command installed with the installation of bridge-utils to configure Linux bridge that we’ll use to configure KVM networking.

Create a new bridge:
$ sudo brctl addbr <virbr0>
$ sudo brctl stp <virbr0> on

Set the bridge device up:
$ sudo ip link set up dev <virbr0>

Add a device to a bridge, for example vnet0:
$ sudo brctl addif <virbr0> <vnet0>

Assigning an IP address:
$ sudo ip addr add dev <virbr0> 192.168.120.1/24 broadcast 192.168.120.255
$ sudo ip route add 192.168.120.0/24 via 192.168.120.1 dev <virbr0>

Show current bridges and what interfaces they are connected to:
$ brctl show
$ brctl showmacs <virbr0>
$ brctl showstp <virbr0>

Delete a bridge, you need to first set it to down:
$ sudo ip link set dev <virbr0> down
$ sudo brctl delbr <virbr0>

[[Step 8]]: Create Linux Bridge using XML
This network configuration uses a Linux bridge in combination with Network Address Translation (NAT) to enable a guest OS to get outbound connectivity regardless of the type of networking (wired, wireless, dial-up, and so on) used in the KVM host without requiring any specific administrator configuration.

The quickest way to get started is by utilizing existing default network configuration. Dump default network xml configuration using below command.
$ sudo virsh net-dumpxml default > mynet.xml

Edit this file accordingly and use it to define new network interface
$ sudo vim mynet.xml

<network>
  <name>mynet</name>
  <forward mode='nat'>
    <nat>
      <port start='1024' end='65535'/>
    </nat>
  </forward>
  <bridge name='virbr0' stp='on' delay='0'/>
  <ip address='192.168.120.1' netmask='255.255.255.0'>
    <dhcp>
      <range start='192.168.120.100' end='192.168.120.200'/>
    </dhcp>
  </ip>
</network>

To define a network from an XML file without starting it, use:
$ sudo virsh net-define mynet.xml

To start a (previously defined) inactive network, use:
$ sudo virsh net-start <mynet>

To create transient network that cannot be set to autostart use:
$ sudo virsh net-create mynet.xml

To set the network to autostart, use:
$ sudo virsh net-autostart <mynet>

Check to Confirm if autostart flag is turned to yes – Persistent should read yes as well.
$ sudo virsh net-list --all

To convert a network name to network UUID – previously defined UUID, use:
$ sudo virsh net-uuid <mynet>

Confirm that the bridge was successfully created, use brctl command provided by bridge-utils package to check available bridges,
$ sudo brctl show <virbr0>

Checking Ip address assigned to the interface, use ip command for this:
$ ip addr show dev <virbr0>

Attaching an interface to a VM, this takes effect immediately, and the NIC will be persistent on further reboots.
$ sudo virsh attach-interface --domain <vm> --type network --source <mynet> --model virtio --config --live/current
$ sudo virsh domiflist <vm>

Detaching an interface attached to a VM
$ sudo virsh detach-interface --domain <vm> --type network --mac <11:22:33:44:55:66> --config --live/current
$ sudo virsh domiflist <vm>

To fully remove a network, First destroy the network to put it in inactive mode:
$ sudo virsh net-destroy <mynet>

Next, undefine the network.
$ sudo virsh net-undefine <mynet>

Confirm that the network is not listed as inactive/active.
$ sudo virsh net-list --all

You can as well use brctl command to check:
$ sudo brctl show <mynet>
